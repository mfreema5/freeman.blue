<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <link href="common/reset.css" media="screen,print" rel="stylesheet">
    <link href="css/screen-sideways-stats.css" rel="stylesheet">
    <title>Sideways statistics</title>
    <meta name="author" content="Michael J. Freeman">
    <meta name="description" content="Sideways stats &ndash; another way to look at the characteristics of linear regressions.">
    <link rel="icon" sizes="16x16 24x24 32x32 48x48 64x64" href="common/gfx/favicon.ico" type="image/x-icon">
    <link rel="icon" sizes="192x192" href="common/gfx/favicon-192x192.png" type="image/png">
    <link rel="icon" sizes="152x152" href="common/gfx/favicon-152x152.png" type="image/png">
    <link rel="icon" sizes="120x120" href="common/gfx/favicon-120x120.png" type="image/png">
    <link rel="shortcut icon" href="common/gfx/favicon.ico" type="image/x-icon" />
    <link href='http://fonts.googleapis.com/css?family=Merriweather+Sans:700,700italic,300,300italic&subset=latin' rel='stylesheet' type='text/css'>
    <script src="css/modernizr.custom.30817.js"></script>
  </head>
  <div id="top-frame">&#8203;</div>
  <div id="base-frame">&#8203;</div>
  <body>
    <main>
      <h1>Sideways statistics</h1>
      <h1>Another way to look at statistical significance</h1>
      <article>
        <h2>Background</h2>
        <p>
          This page is the result of me wanting to understand these four characteristics of linear regressions:
        </p>
        <ol>
          <li>confidence level / alpha</li>
          <li>statistical power / beta</li>
          <li>sample size</li>
          <li>effect size</li>
        </ol>
        <p>
          I wanted to know not only what they are, but how they are related to each other, since they all come up in discussions of statistical significance. More specifically, in discussions of the use of null-hypothesis significance testing as part of empirical studies in the social sciences. I've been obsessed with null-hypothesis significance testing for a while now, to the point where it almost derailed my Master's thesis in technical communication (<a href='http://www.freeman.blue/papers/finding-research-value-thesis-MJF.html'>“Finding Research Value &ndash; the metrics and methods for evaluating research”</a>).
        </p>
        <div class="w1008">
          <img src="images/large/basic-alpha-beta-es-n.svg" alt="Figure 01: A “sideways” graph" />
          <p><b>Figure 01</b> &ndash; Relationship of <em>&alpha;</em>, <em>&beta;</em>, sample size, & effect size<span><a href="images/basic-alpha-beta-es-n.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
        </div>
        <p>
          To be able to make rational arguments about null-hypothesis significance testing, I needed to understand those four inter-related characteristics of linear regressions.  But none of the explanations I was reading in a variety of books were working for me.  Since I'm a visual thinker, I came up with a way to represent the relationship between those four characteristics graphically.
        </p>
        <p>
          I think that I understand them now, but the best way to check if you understand something is to try to explain it to someone else. So, these web-pages are an attempt to explain the graphics I've come up with, and how I use them to understand the four significance-related characteristics of linear regressions.
        </p>
        <section class="digression">
          <h4>Caveat emptor <span class="show-expansion" id='show_caveat'><a onclick="setDisplay('show_caveat','none'); setDisplay('caveat','inline-block')">&nbsp;…&nbsp;</a></span></h4>
          <span class="expansion" id='caveat'><p>
              I've been a chemist, a quality manager, a copy editor, and a specialty gas consultant, and I'm aiming to be a technical writer, but I am not a statistician. You should confirm the validity of anything you read here before applying it. Especially since my interest in the use of these four characteristics of statistical analysis is a narrow one. While I may not be wrong in my understanding of how they apply to linear regressions in the social sciences, my ideas about them may be way off-base in regard to other applications.<a onclick="setDisplay('show_caveat','inline'); setDisplay('caveat','none')">&nbsp;<b>&#x2190;</b>&nbsp;</a>
          </p></span>
        </section>
      </article>
      <article>
        <h2>Sideways statistics show the relationship between two probability curves</h2>
        <section>
          <p>
            I've called this “Sideways statistics” because of the graphics I use to help myself understand the four significance-related characteristics of linear regressions, such as <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure01">Figure 01</a>.
          </p>
          <div class="w1008 left">
            <img src="images/large/Standard_deviation_diagram.svg" alt="Figure 02a: Probability curve" />
            <p><b>Figure 02</b> &ndash; normally distributed probabilities</p>
          </div>
          <p>
            To explain how I got to the graphics such as in <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure01">Figure 01</a>, let's start with a graph of a standard probability curve, <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure02">Figure 02</a> (adapted from <a href="https://commons.wikimedia.org/wiki/File:Standard_deviation_diagram.svg">a graphic</a> on Wikimedia Commons).  I'm assuming you've seen something like it before.  If you haven't, you might want to take at look at the <i><a href="probability-curves-primer.html">Probability Distributions Primer</a></i> before continuing.
          </p>
          <div class="w1008">
            <img src="images/large/basic-prob-alpha.svg" alt="Figure 03: Basic correlation with alpha = 5%" />
            <p><b>Figure 03a</b> &ndash; splitting the total area under the curve into two proportions<span><a href="images/basic-prob-alpha.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
          </div>
          <p>
            The horizontal axis comprises the possible values for some measure. The vertical axis is the probability of getting each value on the horizontal axis when you make a measurement. Typically, however, we aren't interested in the probability of a specific value, but rather we're interested in the probability of getting a result in a range; in other words, a result that is “greater than or equal to” or that is “less than or equal to” some value.
          </p>
          <p>
            The probability of getting a result that is bounded by a specific value is equal to the total area under the curve on the appropriate side of that value. In <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure03a">Figure 03a</a>, the the shaded area is the probability of getting a result that's “less than or equal to” the bounding value.
          </p>
        </section>
        <section>
          <h3>Setting the boundary for alpha</h3>
          <p>
            In the case of linear regression, the probability that comes up most often is the “<i>p</i> value”, a.k.a, “alpha”, “confidence level”, etc. In the social sciences, the most common value for <i>alpha</i> is 5%, often reported as “<i>p</i>&lt;.05”.  The shaded area in <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure03a">Figure 03a</a> is 5% of the total area.
          </p>
          <p>
            When you're using a null-hypothesis significance test to analyze a sample, a confidence level of 95% means that there is a 95% chance that population correlation is not zero. You don't precisely know the population correlation because you measured and analyzed only a sample of the population, instead of the entire population. So, what you've found is the sample correlation, and the population correlation is related to the sample correlation by a probability curve.
          </p>
          <div class="w1536 left">
            <img src="images/large/basic-prob-alpha-sideways.svg" alt="Figure 03b: Sideways correlation with alpha = 5%" />
            <p><b>Figure 03b</b> &ndash; turning a probability curve sideways<span><a href="images/basic-prob-alpha-sideways.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
          </div>
          <p>
            In <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure03a">Figure 03a</a>, we'll say that the mean value of the probability curve is the value of a sample correlation of some analysis. The probability curve therefore shows what values the <em>population</em> correlation may have, based on the <em>sample</em> correlation value. In <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure03a">Figure 03a</a>, 5% of the total area under the curve is bounded by a measured value of zero. So, there is a 5% probability that the population correlation is “less than or equal to” zero. (That's not quite the same as the “null” in “null hypothesis significance testing”, but for now we'll treat it like it is. Everything still works out properly if we do; see: <a href="error-types-sideways.html#symmetry">&ldquo;Sideways Type I and Type II errors &ndash; <i>Symmetry</i>&rdquo;</a>.)
          </p>
          <p>
            You can take the previous example graphic and stand it up on end, as is shown in <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure03b">Figure 03b</a>. Doing so puts the zero/null line at the bottom of the graph, and makes the horizontal axis the probabilities of the curve. Meanwhile, the vertical axis becomes the value of correlations.
          </p>
          <p>
            Why is this is a useful convention?  Because when you're designing a study that will use a linear regression analysis, there are two probability curves involved in the experimental design, and the “sideways” arrangment makes it easier to see the relationship between them, because we can place them next to each other, such as in <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure04">Figure 04</a>.
          </p>
        </section>
        <section>
          <h3 class="before">Two probabilities: alpha and beta</h3>
          <div class="w1008">
            <img src="images/large/basic-prob-alpha-sideways-dual.svg" alt="Figure 04: Comparing two sideways graphs" />
            <p><b>Figure 04</b> &ndash; relating two probability curves<span><a href="images/basic-prob-alpha-sideways-dual.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
          </div>
          <h3 class="after">Two probabilities: alpha and beta</h3>
          <p>
            The first curve that's involved in the design of a statistical experiment describes the sample correlation you're likely to find when you do the analysis.  Its location is determined by the population correlation.  The second curve describes the population correlation.  Its location is determined by the sample correlation that you get from your analysis.
          </p>
          <p>
            The more common way to refer to these two probability curves is through the characteristics  “alpha” and “beta”.  As was mentioned above, <i>alpha</i> is also referred to as the “<i>p</i> value”, or the “confidence level”, etc.  <i>Alpha</i> is a proportion of the population probability curve.
          </p>
          <p>
            The other characteristic, <i>beta</i>, is a proportion of the sample probability curve.  Often, “beta” is described as the proportion of the sample probability curve that is <em>not</em> the “statistical power”.  More simply, ‘1 - &beta; = statistical power’.  In sideways statistics, it's easier to just say that the sample probability curve will be divided into two parts.  One of those is <i>beta</i>, the other is the <i>statistical power</i>.  (And that's why they always add up to 100%&mdash;because each is just one of two proportions of the total area.)
          </p>
          <section class="digression">
            <h4>But I don't know the population correlation, do I?<span class="show-expansion" id="show_popcorr"><a onclick="setDisplay('show_popcorr','none'); setDisplay('popcorr','inline-block')">&nbsp;&hellip;&nbsp;</a></span></small></h4>
            <span class="expansion" id="popcorr"><p>
              It may seem somewhat confusing to talk about the population correlation as defining the location of the sample probability curve, since if you knew the exact population correlation, you wouldn't need to be doing a linear regression to demonstrate what it is.  The thing to keep in mind is that the population correlation exists irrespective of whether or not you know what it is, since that correlation is an intrinsic property of whatever phenomenon it is you're studying.  So, in practice, you typically come up with some way to estimate the population correlation, and then use that estimate to find the <i>beta</i> and the <i>statistical power</i> of your analysis.<a onclick="setDisplay('show_popcorr','inline'); setDisplay('popcorr','none')">&nbsp;<b>&#x2190;</b>&nbsp;</a>
            </p></span>
          </section>
          <p>
            So, the major reason for making these example graphics “sideways” is to make it easier to understand the relationship of these two probability curves. By making them vertical, we can put them “back-to-back” and therefore see how the characteristics of <i>alpha</i>, <i>beta</i>, <i>sample size</i>, and <i>effect size</i> are related to each other.
          </p>
        </section>
      </article>
      <article>
        <h2>The two probability curves in more detail</h2>
        <p>
          The last part of the previous section, that discusses probability curves, along with <i>alpha</i> and <i>beta</i> is not, for me at least, intuitively obvious, so let's go over it again.  (If it already makes sense to you, you might skip ahead to <em><a href="#alpha-beta-effects">&lsquo;Confidence level &amp; statistical power&rsquo;</a></em>.)
        </p>
        <section>
          <h3 class="before">Sample correlation &rarr; Population probability</h3>
          <div class="w1008 left">
            <img src="images/large/population-probability.svg" alt="Figure 05: Population probability around sample correlation" />
            <p><b>Figure 05</b> &ndash; <i>Sample</i> correlation bisects <i>population</i> probability curve<span><a href="images/population-probability.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
          </div>
          <h3 class="after">Sample correlation &rarr; Population probability</h3>
          <p>
            Let's say you measure a variety of characteristics of a presumably representative sample of a population. You can do a linear regression analysis of the measurements, and precisely calculate the correlations between the characteristics you measured. But, your results are only precise descriptions of the sample. They aren't precise descriptions of the population, because there is always some amount of error, some amount of potential discrepancy, between the mean value of some characteristic for a sample, and the mean value of that characteristic for the population.
          </p>
          <p>
            So, you can't precisely determine the correlation between characteristics for the population. But what you can determine precisely is the probability that the population's correlation falls within some range of values. In the case of null-hypothesis significance testing, your goal is to calculate the probability that the population correlation is not zero. When you report that a correlation is, e.g., “statistically significant at ‘p&lt;.05’”, you're reporting the probability that you calculated—based on the sample correlation—that the magnitude of the population's correlation is greater than zero. In the case of ‘p&lt;.05’, that probability is 0.95, in other words, 95%.
          </p>
          <p>
            You can think about that probability calculation as being a probability curve that you draw around the sample correlation. In <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure05">Figure 05</a>, the horizontal line that splits the curve marks the value of the sample correlation. The probability curve itself, however, describes the correlation of the population.
          </p>
          <p>
            This is an important distinction to understand. In <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure05">Figure 05</a>, the straight line is about the <em>sample</em>. The curve is about the <em>population</em>.
          </p>
        </section>
        <section>
          <h3 class="before">Population correlation &rarr; Sample probability</h3>
          <div class="w1008">
            <img src="images/large/sample-probability.svg" alt="Figure 06: Sample probability around population correlation" />
            <p><b>Figure 06</b> &ndash; <i>Population</i> correlation bisects <i>sample</i> probability curve<span><a href="images/sample-probability.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
          </div>
          <h3 class="after">Population correlation &rarr; Sample probability</h3>
          <p>
            <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure06">Figure 06</a> is the opposite of that. The straight line is about the <em>population</em>. The curve is about the <em>sample</em>. They are both on the opposite side of the vertical axis from the previous graph because they show the opposite relationship.
          </p>
          <p>
            What is that relationship? It's the probability of finding some particular value for a sample correlation, based on what the population correlation is. Even though you may not be able to ever directly measure the population correlation, it does exist and it does have some value. And that value establishes the probabilities of the sample correlation value you'll find.
          </p>
        </section>
        <section>
            <h3>Population probability &harr; alpha<br />Sample probability &harr; beta</h3>
          <p>
            The sample probability curve is described by <i>beta</i>, and/or “statistical power”.  The population probability curve, which was in <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure05">Figure 05</a>, is described by <i>alpha</i>, a.k.a. “confidence level” or “<i>p</i> value”.
          </p>
        </section>
      </article>
      <article id="alpha-beta-effects">
        <h2 class="before">Relation of alpha and beta</h2>
        <div class="w1008">
          <img src="images/large/basic-alpha-beta-es-n.svg" alt="Figure 08: Basic relationship of alpha and beta" />
          <p>Figure 08<span><a href="images/basic-alpha-beta-es-n.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
        </div>
        <h2 class="after">Relation of alpha and beta</h2>
        <p>
          Beta describes the sample probability curve by telling you where it is in relation to the population probability curve.  This is why it's useful to consider the two probability curves back-to-back, which is what we can do once we turn things sideways.  In null-hypothesis significance testing, alpha tells you where the population probability curve is in relation to the null line.  But when testing for significance against some other hypothesis, alpha becomes like beta, in that it tells you where the population probability curve is in relation to the sample probability curve.
        </p>
        <p>
          <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#Figure08">Figure 08</a> is an example of how sideways statistics shows the relationship of alpha and beta for a statistical experiment that uses a null-hypothesis significance test.  The curve on the <strong>right</strong> is the <strong>population</strong> probability curve, and the shaded area is equal to <strong><i>alpha</i></strong>. The curve on the <strong>left</strong> is the <strong>sample</strong> probability curve, and the shaded portion of its area is equal to the <strong><i>statistical power</i></strong> of the analysis.
        </p>
        <p>
          On the <strong>right</strong> side of the graphic, which has the population probability curve, the value of <i>alpha</i> is equal to the area bounded by the null line, because of the symmetry of Type I errors in a null-hypothesis test.  (See: <i><a href="error-types-sideways.html#type-i-errors" >Sideways Type I errors</a></i>.)  Any sample correlation that is higher than the line bisecting the population probability curve would be statistically significant at “<i>p</i>&lt;.05”.
        </p>
        <p>
          On the <strong>left</strong> side of the graphic, which has the sample probability curve, the area between that curve and the vertical baseline is divided into two portions by the line that marks the minimum statistically-significant sample correlation.  The shaded portion of its area above that line equals the <i>statistical power</i> of the analysis. The unshaded area equals <i>beta</i>.  If we assume that the unshaded portion of the area is 40% of the total area, then <i>beta</i> is 0.40, and the <i>statistical power</i> is 60%.  This means that if we take a sample of that population and test it for a correlation, there is a 60% chance that we will find a statistically significant correlation (at &ldquo;<i>p</i>&lt;.05&rdquo;).
        </p>
        <p>
          In other words, the fact that 60% of the sample correlation probability curve is higher than the minimum statistically significant sample correlation value (<i>p</i>&lt;.05), means that there is a 60% chance that the sample correlation we find will be a value that is equal to or greater than that minimum value.  The relationship of the two probability curves shows the relationship of <i>alpha</i> and <i>beta</i> (which is the inverse of <i>statistical power</i>).
        </p>
      </article>
      <article>
        <h2>Sample size shapes the probability curve</h2>
        <div class="w1008">
          <img src="images/large/rising-N-overlap.svg" alt="Figure NN: A higher ‘N’ tightens the probability curve " />
          <p>Figure NN<span><a href="images/rising-N-overlap.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
        </div>
        <p>
          The shape of the probability curves in sideways statistics is influenced by the number of cases in the sample.  For relatively small sample sizes, the probability curves will be short and widely spread.  For large sample sizes, the probability curves will be tall and narrow.  <a class= "framejump" target="Mframe" href="main-sideways-graphics.html#FigureNN">Figure NN</a> shows the narrowing shape of a probability curves as the sample size rises.
        </p>
        <p>
          In a study that uses a null hypothesis, <i>alpha</i> describes where the population probability curve is in relation to the null line.  The exact middle of that curve marks the minimum statistically significant sample correlation.  Since sample size determines how narrow or wide a population probability curve is, together alpha and sample size determine the magnitude of the minimum statistically significant sample correlation for a study.  (See: <i><a href="sample-size-impact.html">The impact of sample size</a></i>.)
        </p>
      </article>
      <article>
        <h2>Effect size &ndash; final piece of the puzzle</h2>
        <p>
          <i>Alpha</i> determines the placement of the population probability curve. The shape of the population probability curve is determined by <i>sample size</i>. Between <i>sample size</i> and <i>alpha</i>, the minimum sample correlation that is statistically significant is determined. And that minimum significant correlation will divide the area under the sample probability curve and thus define the <i>statistical power</i> of the analysis.
        </p>
        <p>
          What defines the placement of the sample probability curve on the graph is the <i>effect size</i> (a.k.a., “size of effect”).  Which, up until now, I've referred to as simply “the population correlation”, in order to maintain the symmetry between the names for things on the left and right side of a sideways graph.  The <i>effect size</i> isn't normally referred to as the population correlation, because if you're doing a significance test, you'll never precisely know what the population correlation is.  The <i>effect size</i> is an <strong>estimate</strong> of the population correlation you expect to exist.  Typically that estimation is done using previous studies of similar phenomena.
        </p>
        <p>
          The sample population curve is placed so that its centerline matches the effect size.  Once that's done, the <i>statistical power</i> for the analysis is determined, since now you can find out where the minimum significant correlation slices through the area under the sample probability curve.
        </p>
      </article>
      <article>
        <h2>Conclusion</h2>
        <p>
          Sideways statistics uses the relationship of the two probability curves to show how <i>alpha</i>, <i>beta</i>, <i>sample size</i>, and <i>effect size</i> are interlocked.  The shape of the curves is determined by the <i>sample size</i>.  The placement of the curve on the right is determined by <i>alpha</i>.  The placement of the curve on the left is determined by <i>beta</i>.  And the <i>effect size</i> determines the placement of the curve on the left.  If any one of those characteristics changes, then at least one of the others must also change, because of the way the curves, their bisecting lines, and the null line are all woven together.
        </p>
        <p>
          Sideways statistics can also be a tool to help improve our understanding of the issues related to significance testing of linear regressions.  The following pages cover some of those issues:
        </p>
        <ul>
          <li>
            The interaction of <i>alpha</i> and <i>beta</i> &ndash; <a href="alpha-beta-interaction.html">link</a>
          </li>
          <li>
            The impact of sample size &ndash; <a href="sample-size-impact.html">link</a>
          </li>
          <li>
            The impact of effect size &ndash; <a href="effect-size-impact.html">link</a>
          </li>
          <li>
            Is a result that's insignificant compared to null always unimportant? &ndash; <a href="insignificance-is-not-failure.html">link</a>
          </li>
          <li>
            How to do an upper-bound significance test &ndash; <a href="upper-bound-hypothesis.html">link</a>
          </li>
        <ul>
      </article>
    </main>
    <footer>
      <object class="nav" data="nav-frame-sideways-stats.html" type="text/html" />
    </footer>
    <script>
      if(!Modernizr.svg)for(var imgs=document.getElementsByTagName("img"),svgExtension=/.*\.svg$/,l=imgs.length,i=0;i<l;i++)imgs[i].src.match(svgExtension)&&(imgs[i].src=imgs[i].src.slice(0,-3)+"png",console.log(imgs[i].src));
      function setDisplay(b,c){var a=document.getElementById(b);a.style.display=a.style.display=c};
    </script>
  </body>
</html>
