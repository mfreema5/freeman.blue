<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <link href="common/reset.css" media="screen,print" rel="stylesheet">
    <link href="css/screen-sideways-stats.css" rel="stylesheet">
    <title>Upper-bound significance testing</title>
    <meta name="author" content="Michael J. Freeman">
    <meta name="description" content="Using sideways stats to understand statistics.">
    <link rel="icon" sizes="16x16 24x24 32x32 48x48 64x64" href="common/gfx/favicon.ico" type="image/x-icon">
    <link rel="icon" sizes="192x192" href="common/gfx/favicon-192x192.png" type="image/png">
    <link rel="icon" sizes="152x152" href="common/gfx/favicon-152x152.png" type="image/png">
    <link rel="icon" sizes="120x120" href="common/gfx/favicon-120x120.png" type="image/png">
    <link rel="shortcut icon" href="common/gfx/favicon.ico" type="image/x-icon" />
    <link href='http://fonts.googleapis.com/css?family=Merriweather+Sans:700,700italic,300,300italic&subset=latin' rel='stylesheet' type='text/css'>
    <script src="css/modernizr.custom.30817.js"></script>
  </head>
  <div id="top-frame">&#8203;</div>
  <div id="base-frame">&#8203;</div>
  <body>
    <main>
      <h1>Upper-bound significance testing</h1>
      <h1>One of the alternatives to checking against null</h1>
      <article>
        <h2>Testing for importance using significance</h2>
        <p>
          As part of the designing a study that will use linear regression analysis, you should consider what the best hypothesis against which to test for significance would be.  You want a hypothesis that fits both the theory you're intending to support or debunk, and that works with the sort of data that you're expecting to have.
        </p>
        <p>
          The page on this site titled <a href="insignificance-is-not-failure.html">“Insignificance is not failure”</a> discusses why null-hypothesis significance testing is not really the universal litmus test for significance that so many researchers in the humanties treat is as.  An example of an alternative test given on that page is an upper-bound significance test, one in which you're attempting to demonstrate that a correlation is too small in effect to be important.  This page looks at that sort of analysis in a little more detail.
        </p>
        <p>
          What would it look like if we designed our experiment from the beginning to check for an upper bound? That is, what if we were trying to demonstrate that, with 95% confidence, a correlation is lower than some value that we have interpreted as the minimum <b>important</b> correlation?
        </p>
        <div class="w1008 left">
          <img src="images/large/upper-bound-test.svg" alt="Figure 01: Statistical experiment to check upper bound" />
          <p>Figure 01<span><a href="images/upper-bound-test.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
        </div>
        <p>
          First, we'd mark our upper bound&mdash;which equates to the minimum important correlaton&mdash;on the the vertical baseline of our ‘sideways’ graph of the analysis.  Then, the population probability curve on the right side of the graph would be positioned so that 95% of the area between it and the vertical baseline would be below that minimum important correlation. That would mean that line that bisects the population probability curve is therefore the maximum sample correlation that demonstrates that the population correlation does not meet your criteria for importance, with ‘<i>p</i>&lt;.05’. See <a class= "framejump" target="Gframe" href="upper-bound-graphics.html#Figure01">Figure 01</a>.
        </p>
        <p>
          As with null-hypothesis testing, the sample population curve on the left side of the ‘sideways’ graph is centered on the population correlation we're expecting to see.  The farther away from the minimum important correlation that expected size of effect is, the better off we are, as can be seen in <a class= "framejump" target="Gframe" href="upper-bound-graphics.html#Figure01">Figure 01</a>, since the shaded area under the sample probability curve on the left side of the graph is the <em>statistical power</em> of the analysis.  The statistical power is the probability of <b>not</b> committing a Type II error (discussed more below) and therefore the larger it is, the better.  And the way to make that shaded area in <a class= "framejump" target="Gframe" href="upper-bound-graphics.html#Figure01">Figure 01</a> larger would be to move the sample probability curve down, away from the upper bound.  In this example, the statistical power is a somewhat dismal 16%.
        </p>
        <p>
          So, arranging things on a sideways graph to help understand an upper-bound significant test is fairly straight-forward.  However, when it comes to doing the actual calculation things become somewhat more complicated.  More specifically, they are less symmetric.
        </p>
        <div class="w1008">
          <img src="images/large/TruncationFades.svg" alt="Figure 02: Probability curves distorted by truncation" />
          <p>Figure 02<span><a href="images/TruncationFades.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
        </div>
        <p>
          When considering correlations near zero (which is where null-hypothesis significance testing is most relevant), you can assume that the probability distributions are symmetric about some mean value.  But when looking at larger correlations, you can no longer assume that.
        </p>
        <p>
          This is because correlations can never have a magnitude above 1.  So, as the correlation of interest moves higher up the vertical baseline, the probability distribution begins to “pile up” against the maximum value of 1, and this distorts the curve, making it asymmetric.
        </p>
        <p>
          <a class= "framejump" target="Gframe" href="upper-bound-graphics.html#Figure02">Figure 02</a> shows this.  The line marking the mean of the curve no longer perfectly bisects the area of the sample correlation probability curve.  When one side of the curve is truncated, the probability of the remaining values increase, distorting the distribution of area about the mean.  Which means we can no longer assume that 50% of the area is on either side of the correlation that defines the curve.
        </p>
        <p>
          To do the calculations, you have to correct for this distorting truncation to the probability curves.  You can do this by transforming the data; typically you would transform each correlation into its Fisher <em>z</em>-score, using the following equation.
        </p>
        <div class="w605 left">
          <img src="images/large/Ztransformation.svg" alt="Figure 03: Fisher transformation stretching correlations to infinity" />
          <p>Figure 03<span><a href="images/Ztransformation.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
        </div>
        <div class="maths">
          <img src="images/large/Zformula.svg" alt="Fisher transformation equation" />
        </div>
        <p>
          What the Fisher transformation does it to effectively remove the upper and lower bounds&mdash;at +1 and -1&mdash;that are a consequence of the nature of correlations.  <a class= "framejump" target="Gframe" href="upper-bound-graphics.html#Figure03">Figure 03</a> shows one way of understanding what the transformation does.  The perfectly straight diagonal line represents untransformed correlations; it's the line where <i>r</i>=<i>r</i>.  The long curving line is shows the results of Fisher transformations; it's the line where <i>r</i>=<i><b>z</b>(r)</i>.  Near the origin the two lines are almost identical, but as the correlation magnitude approaches 1, the transformed values split away and extend toward infinity.  This effectively removes the truncating bounds, so the probability curves have nothing to “bunch up” against.
        </p>
      </article>
      <article>
        <h2>Type I errors</h2>
        <div class="w1008">
          <img src="images/large/upper-bound-type-I-error.svg" alt="Figure 04: Type I error in an upper bound test" />
          <p>Figure 04<span><a href="images/upper-bound-type-I-error.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
        </div>
        <p>
          The visual interpretation of Type I errors for this type of analysis differs from that for a null-hypothesis significance test.  For a Type I error in a null-hypothesis significance test, the population is assumed to have a correlation of zero.  While the same sort of wort-case scenario is used in an upper-bound significance test for understanding Type I errors, the specific worst-case used is different.  For a Type I error, instead of being equal to <i>null</i>, the worst-case population correlation is assumed to be infinitesimally higher than the correlation selected as the boundary of importance.
        </p>
        <p>
          Type I errors are false positives.  In this case, that means that you would identify a population correlation as being below the minimum important correlation, when in fact it's equal to or above that minimum.  <a class= "framejump" target="Gframe" href="upper-bound-graphics.html#Figure02">Figure 02</a> shows a population correlation that is infinitesimally higher than the minimum important correlation.  The line that bisects the population probability curve is the maximum correlation that would meet your hypothesis that the correlation is not important.  So, the area below that line, and between the sample probability curve and the vertical baseline, is equal to the worst-case probability of getting an apparently unimportant sample correlation for a population that, in fact, has an important correlation.
        </p>
        <p>
          Much like with the null-hypothesis significance test, we know that the area that covers the Type I error is 5%, since it is symmetric with the area that's between the population probability curve and the vertical baseline, and also above  the value chosen as the minimum important population correlation.  The sample population curve was positioned vertical such that the area above the minimum important correlation was 5% of the total, giving the analysis ‘p<.05’.  Both the 5% areas are shaded in <a class= "framejump" target="Gframe" href="upper-bound-graphics.html#Figure04">Figure 04</a>.
        </p>
      </article>
      <article>
        <h2>Type II errors</h2>
        <div class="w1008 left">
          <img src="images/large/upper-bound-type-II-error.svg" alt="Figure 05: Type II error in an upper bound test" />
          <p>Figure 05<span><a href="images/upper-bound-type-II-error.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
        </div>
        <p>
          Type II errors are false negatives.  In this case that means you would fail to properly identify a population correlation as being below the minimum important correlation.  <a class= "framejump" target="Gframe" href="upper-bound-graphics.html#Figure05">Figure 05</a> shows the Type II error in our example test that gave us <a class= "framejump" target="Gframe" href="upper-bound-graphics.html#Figure05">Figure 01</a>.  The difference in Figure 05 is that instead of the statistical power being shaded, the area equal to <i>beta</i> is shaded.  Any sample correlation that is higher than the maximum correlation that is significantly below the importance cut-off will lead to a Type II error.  The population does have an unimportant correlation, but the sample analysis fails to identify it.
        </p>
        <p>
          One important thing to note is that the two areas related to Type II errors in these tests are reversed from those for Type II errors with null-hypothesis significance tests.  In the latter, the statistical power was the area farther from the null-line.  In upper-bound tests, the statistical power is the area nearer to the null-line.  This is because the null-hypothesis test are effectively a specialized sort of <em>lower</em>-bound test&mdash;one in which the lower-bound is equal to zero.
        </p>
        <div class="w1008">
          <img src="images/large/TruncationBeta.svg" alt="Figure 06: Type II error for truncated probability curves" />
          <p>Figure 06<span><a href="images/TruncationBeta.html"><img class="button" src="images/full-screen-arrow.svg" alt="Go to full size" /></a></span></p>
        </div>
        <p>
          Of course, these figures are assuming that the correlations have undergone a Fisher transformation. For comparison purposes, <a class= "framejump" target="Gframe" href="upper-bound-graphics.html#Figure06">Figure 06</a> is an example of untransformed correlations.  It's also a different analysis that the one used in the earlier figures, and shows a statistical power of 39.2%.  What's particularly interesting is that the expected population correlation has been moved up to not only be close to +1, but is also equal to the upper-bound.  Without the truncation inherent in correlation analyses, the statistical power would have been only 5%. But since that inherent truncation has an uneven effect on the two non-transformed probability curves, it means that they are no longer symmetric, so their respective mean lines no longer slice through identical areas at identical locations, so the areas bounded by them are no longer equal.
        </p>
        <p>
          So, while with non-truncated curves <i>alpha</i> would have been exactly equal to the statistical power, at 5%, with the truncation, the statistical power has grown to almost 40%.  Presumably, this increase would continue as the upper-bound approached the maximum possible correlation of +1, and should achieve 100% when the upper-bound equals +1.  (Which makes sense&mdash;<em>every</em> untransformed correlation is at or below 1 in magnitude.)
        </p>
      </article>
      <nav>
        <object class="nav" data="nav-frame-sideways-stats.html" type="text/html"></object>
        <div style="clear: both;"></div>
      </nav>
    </main>
    <footer></footer>
    <script>
      if(!Modernizr.svg)for(var imgs=document.getElementsByTagName("img"),svgExtension=/.*\.svg$/,l=imgs.length,i=0;i<l;i++)imgs[i].src.match(svgExtension)&&(imgs[i].src=imgs[i].src.slice(0,-3)+"png",console.log(imgs[i].src));
      function setDisplay(b,c){var a=document.getElementById(b);a.style.display=a.style.display=c};
    </script>
  </body>
</html>
